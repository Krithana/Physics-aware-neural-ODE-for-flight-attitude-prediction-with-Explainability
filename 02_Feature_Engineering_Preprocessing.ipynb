{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚙️ Notebook 2: Feature Engineering & Preprocessing\n",
    "## Residual Physics-Aware Neural ODE for Flight Attitude Prediction\n",
    "\n",
    "**Input from Notebook 1:**\n",
    "- 100 trajectories\n",
    "- 7.7M timesteps  \n",
    "- 8 aircraft types\n",
    "\n",
    "**This Notebook:**\n",
    "1. Load data from Notebook 1\n",
    "2. Engineer physics features\n",
    "3. Compute derivatives  \n",
    "4. Normalize data\n",
    "5. Create sequences\n",
    "6. Split train/val/test\n",
    "7. Export for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from Notebook 1...\n",
      "✓ Loaded 100 trajectories\n",
      "✓ DataFrame: (7684932, 18)\n",
      "✓ Timesteps: 7,684,932\n",
      "✓ Sampling rate: 0.1 sec\n"
     ]
    }
   ],
   "source": [
    "# Load from Notebook 1\n",
    "print(\"Loading data from Notebook 1...\")\n",
    "\n",
    "with open('./data/raw/opensky_trajectories.pkl', 'rb') as f:\n",
    "    trajectories = pickle.load(f)\n",
    "\n",
    "df_full = pd.read_csv('./data/raw/opensky_flight_data.csv')\n",
    "\n",
    "with open('./data/raw/metadata.pkl', 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(trajectories)} trajectories\")\n",
    "print(f\"✓ DataFrame: {df_full.shape}\")\n",
    "print(f\"✓ Timesteps: {len(df_full):,}\")\n",
    "print(f\"✓ Sampling rate: {metadata['sampling_rate']} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Physics Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Aircraft parameters defined\n"
     ]
    }
   ],
   "source": [
    "# Aircraft parameters (scaled by type)\n",
    "AIRCRAFT_PARAMS = {\n",
    "    'mass': 1200.0,\n",
    "    'Ixx': 1285.0,\n",
    "    'Iyy': 3200.0,\n",
    "    'Izz': 4160.0,\n",
    "    'rho': 1.225,\n",
    "    'g': 9.81\n",
    "}\n",
    "\n",
    "AIRCRAFT_SCALE = {\n",
    "    'B738': 2.5, 'B737': 2.5, 'A320': 2.3,\n",
    "    'A321': 2.6, 'B77W': 5.0, 'A319': 2.2,\n",
    "    'B763': 3.5, 'A333': 4.0\n",
    "}\n",
    "\n",
    "print(\"✓ Aircraft parameters defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Feature engineering function defined\n"
     ]
    }
   ],
   "source": [
    "def add_physics_features(df, aircraft_type='B738'):\n",
    "    \"\"\"Add energy, momentum, Euler angles, dynamic pressure\"\"\"\n",
    "    scale = AIRCRAFT_SCALE.get(aircraft_type, 2.5)\n",
    "    mass = AIRCRAFT_PARAMS['mass'] * scale\n",
    "    Ixx = AIRCRAFT_PARAMS['Ixx'] * scale**2\n",
    "    Iyy = AIRCRAFT_PARAMS['Iyy'] * scale**2\n",
    "    Izz = AIRCRAFT_PARAMS['Izz'] * scale**2\n",
    "    \n",
    "    I_matrix = np.diag([Ixx, Iyy, Izz])\n",
    "    omega = df[['p', 'q', 'r']].values\n",
    "    \n",
    "    # Energy\n",
    "    E_rot = np.array([0.5 * omega[i] @ I_matrix @ omega[i] for i in range(len(omega))])\n",
    "    E_trans = 0.5 * mass * df['V']**2\n",
    "    \n",
    "    # Angular momentum\n",
    "    L = np.array([I_matrix @ omega[i] for i in range(len(omega))])\n",
    "    \n",
    "    # Euler angles from quaternion\n",
    "    q0, q1, q2, q3 = df['q0'], df['q1'], df['q2'], df['q3']\n",
    "    phi = np.arctan2(2*(q0*q1 + q2*q3), 1 - 2*(q1**2 + q2**2))\n",
    "    theta = np.arcsin(np.clip(2*(q0*q2 - q3*q1), -1, 1))\n",
    "    psi = np.arctan2(2*(q0*q3 + q1*q2), 1 - 2*(q2**2 + q3**2))\n",
    "    \n",
    "    # Dynamic pressure\n",
    "    q_dyn = 0.5 * AIRCRAFT_PARAMS['rho'] * df['V']**2\n",
    "    \n",
    "    # Add to dataframe\n",
    "    df['E_rot'] = E_rot\n",
    "    df['E_trans'] = E_trans\n",
    "    df['E_total'] = E_rot + E_trans\n",
    "    df['Lx'] = L[:, 0]\n",
    "    df['Ly'] = L[:, 1]\n",
    "    df['Lz'] = L[:, 2]\n",
    "    df['L_mag'] = np.linalg.norm(L, axis=1)\n",
    "    df['phi'] = phi\n",
    "    df['theta'] = theta\n",
    "    df['psi'] = psi\n",
    "    df['q_dyn'] = q_dyn\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"✓ Feature engineering function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering features for 7.7M timesteps...\n",
      "This will take 2-3 minutes...\n",
      "\n",
      "  10/100\n",
      "  20/100\n",
      "  30/100\n",
      "  40/100\n",
      "  50/100\n",
      "  60/100\n",
      "  70/100\n",
      "  80/100\n",
      "  90/100\n",
      "  100/100\n",
      "\n",
      "✓ Added 11 features\n",
      "  Total columns: 29\n"
     ]
    }
   ],
   "source": [
    "# Apply to all trajectories\n",
    "print(\"Engineering features for 7.7M timesteps...\")\n",
    "print(\"This will take 2-3 minutes...\\n\")\n",
    "\n",
    "df_list = []\n",
    "for traj_id in df_full['trajectory_id'].unique():\n",
    "    df_traj = df_full[df_full['trajectory_id'] == traj_id].copy()\n",
    "    aircraft = df_traj['aircraft_type'].iloc[0]\n",
    "    df_traj = add_physics_features(df_traj, aircraft)\n",
    "    df_list.append(df_traj)\n",
    "    \n",
    "    if (traj_id + 1) % 10 == 0:\n",
    "        print(f\"  {traj_id + 1}/100\")\n",
    "\n",
    "df_features = pd.concat(df_list, ignore_index=True)\n",
    "print(f\"\\n✓ Added {df_features.shape[1] - df_full.shape[1]} features\")\n",
    "print(f\"  Total columns: {df_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Compute Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing derivatives...\n",
      "  10/100\n",
      "  20/100\n",
      "  30/100\n",
      "  40/100\n",
      "  50/100\n",
      "  60/100\n",
      "  70/100\n",
      "  80/100\n",
      "  90/100\n",
      "  100/100\n",
      "\n",
      "✓ Derivatives computed\n"
     ]
    }
   ],
   "source": [
    "def compute_derivatives(df_traj, dt=0.1):\n",
    "    \"\"\"Compute state derivatives for physics loss\"\"\"\n",
    "    state_cols = ['q0', 'q1', 'q2', 'q3', 'p', 'q', 'r', 'V', 'alpha', 'beta']\n",
    "    for col in state_cols:\n",
    "        df_traj[f'{col}_dot'] = np.gradient(df_traj[col].values, dt)\n",
    "    return df_traj\n",
    "\n",
    "print(\"Computing derivatives...\")\n",
    "df_list = []\n",
    "for traj_id in df_features['trajectory_id'].unique():\n",
    "    df_traj = df_features[df_features['trajectory_id'] == traj_id].copy()\n",
    "    df_traj = compute_derivatives(df_traj, metadata['sampling_rate'])\n",
    "    df_list.append(df_traj)\n",
    "    if (traj_id + 1) % 10 == 0:\n",
    "        print(f\"  {traj_id + 1}/100\")\n",
    "\n",
    "df_with_derivs = pd.concat(df_list, ignore_index=True)\n",
    "print(f\"\\n✓ Derivatives computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data scaled\n",
      "✓ Scalers saved\n"
     ]
    }
   ],
   "source": [
    "STATE_COLS = ['q0', 'q1', 'q2', 'q3', 'p', 'q', 'r', 'V', 'alpha', 'beta']\n",
    "CONTROL_COLS = ['elevator', 'aileron', 'rudder', 'throttle']\n",
    "FEATURE_COLS = ['E_total', 'E_rot', 'E_trans', 'L_mag', 'Lx', 'Ly', 'Lz', 'phi', 'theta', 'psi', 'q_dyn']\n",
    "DERIVATIVE_COLS = [f'{c}_dot' for c in STATE_COLS]\n",
    "\n",
    "# Use RobustScaler for real data (handles outliers)\n",
    "state_scaler = RobustScaler()\n",
    "control_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "feature_scaler = RobustScaler()\n",
    "derivative_scaler = RobustScaler()\n",
    "\n",
    "df_scaled = df_with_derivs.copy()\n",
    "df_scaled[STATE_COLS] = state_scaler.fit_transform(df_with_derivs[STATE_COLS])\n",
    "df_scaled[CONTROL_COLS] = control_scaler.fit_transform(df_with_derivs[CONTROL_COLS])\n",
    "df_scaled[FEATURE_COLS] = feature_scaler.fit_transform(df_with_derivs[FEATURE_COLS])\n",
    "df_scaled[DERIVATIVE_COLS] = derivative_scaler.fit_transform(df_with_derivs[DERIVATIVE_COLS])\n",
    "\n",
    "print(\"✓ Data scaled\")\n",
    "\n",
    "# Save scalers\n",
    "with open('./data/processed/scalers.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'state': state_scaler,\n",
    "        'control': control_scaler,\n",
    "        'feature': feature_scaler,\n",
    "        'derivative': derivative_scaler\n",
    "    }, f)\n",
    "print(\"✓ Scalers saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Create Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 30586 sequences\n",
      "  Length: 500 steps (50 sec)\n",
      "  Overlap: 50%\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(df, traj_ids, seq_len=500, stride=250):\n",
    "    \"\"\"Create overlapping sequences\"\"\"\n",
    "    sequences = []\n",
    "    for traj_id in traj_ids:\n",
    "        df_traj = df[df['trajectory_id'] == traj_id].reset_index(drop=True)\n",
    "        if len(df_traj) < seq_len:\n",
    "            continue\n",
    "        \n",
    "        for start in range(0, len(df_traj) - seq_len + 1, stride):\n",
    "            end = start + seq_len\n",
    "            sequences.append({\n",
    "                'states': df_traj.loc[start:end-1, STATE_COLS].values,\n",
    "                'controls': df_traj.loc[start:end-1, CONTROL_COLS].values,\n",
    "                'features': df_traj.loc[start:end-1, FEATURE_COLS].values,\n",
    "                'derivatives': df_traj.loc[start:end-1, DERIVATIVE_COLS].values,\n",
    "                'times': df_traj.loc[start:end-1, 'time'].values,\n",
    "                'traj_id': traj_id,\n",
    "                'aircraft': df_traj['aircraft_type'].iloc[0]\n",
    "            })\n",
    "    return sequences\n",
    "\n",
    "all_ids = df_scaled['trajectory_id'].unique()\n",
    "all_seqs = create_sequences(df_scaled, all_ids, 500, 250)\n",
    "print(f\"✓ Created {len(all_seqs)} sequences\")\n",
    "print(f\"  Length: 500 steps (50 sec)\")\n",
    "print(f\"  Overlap: 50%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences:\n",
      "  Train: 21779\n",
      "  Val: 4365\n",
      "  Test: 4442\n"
     ]
    }
   ],
   "source": [
    "# Split by trajectory to avoid leakage\n",
    "train_ids, temp_ids = train_test_split(list(all_ids), test_size=0.3, random_state=42)\n",
    "val_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=42)\n",
    "\n",
    "train_seqs = [s for s in all_seqs if s['traj_id'] in train_ids]\n",
    "val_seqs = [s for s in all_seqs if s['traj_id'] in val_ids]\n",
    "test_seqs = [s for s in all_seqs if s['traj_id'] in test_ids]\n",
    "\n",
    "print(f\"Sequences:\")\n",
    "print(f\"  Train: {len(train_seqs)}\")\n",
    "print(f\"  Val: {len(val_seqs)}\")\n",
    "print(f\"  Test: {len(test_seqs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All data saved\n",
      "\n",
      "============================================================\n",
      "NOTEBOOK 2 COMPLETE\n",
      "============================================================\n",
      "\n",
      "Ready for Notebook 3: Model Training\n"
     ]
    }
   ],
   "source": [
    "with open('./data/processed/train_sequences.pkl', 'wb') as f:\n",
    "    pickle.dump(train_seqs, f)\n",
    "with open('./data/processed/val_sequences.pkl', 'wb') as f:\n",
    "    pickle.dump(val_seqs, f)\n",
    "with open('./data/processed/test_sequences.pkl', 'wb') as f:\n",
    "    pickle.dump(test_seqs, f)\n",
    "\n",
    "metadata_prep = {\n",
    "    'state_cols': STATE_COLS,\n",
    "    'control_cols': CONTROL_COLS,\n",
    "    'feature_cols': FEATURE_COLS,\n",
    "    'derivative_cols': DERIVATIVE_COLS,\n",
    "    'train_ids': train_ids,\n",
    "    'val_ids': val_ids,\n",
    "    'test_ids': test_ids,\n",
    "    'seq_len': 500,\n",
    "    'sampling_rate': 0.1\n",
    "}\n",
    "\n",
    "with open('./data/processed/metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata_prep, f)\n",
    "\n",
    "print(\"✓ All data saved\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NOTEBOOK 2 COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nReady for Notebook 3: Model Training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
